

# ARTS
## Algorithm
[134 LRU]([https://www.lintcode.com/problem/lru-cache/description](https://www.lintcode.com/problem/lru-cache/description))
Design and implement a data structure for Least Recently Used (LRU) cache. It should support the following operations: `get` and `set`.

`get(key)` - Get the value (will always be positive) of the key if the key exists in the cache, otherwise return -1.  
`set(key, value)` - Set or insert the value if the key is not already present. When the cache reached its capacity, it should invalidate the least recently used item before inserting a new item.


###  Solution1
```python
from collections import deque
class LRUCache:
    def __init__(self,capacity):
        self.dic = {}
        self.capacity = capacity 
        self.deq = deque([])

    
    def get(self,key):
        if key not in self.dic:
            return -1   
        else:
            self.deq.remove(key)
            self.deq.append(key)
            return self.dic[key]

    def set(self,key, value):
        if key in self.dic:
            self.deq.remove(key)
            self.deq.append(key)
            self.dic[key] = val 
        else:
            if len(self.dic) == self.capacity:
                val = self.deq.popleft()
                self.deq.append(key)
                self.dic[key] = value 
                self.dic.pop(val)
            else:
                self.deq.append(key)
                self.dic[key] = value 
                

```
### Explanation
- 使用一个queue保存当前缓存的值，当有值被访问，将其删除，然后再插入到队列首部
- 当队列满后，如果插入重复的值，将其删除；如果没有重复的值，删除队列尾部的值。
- time complexity O(n^2)

### Solution2
```python
class Node:
    def __init__(self,_key, _value):
        self.key = _key 
        self.value = _value 
        self.pre = self.next = None 



class LRUCache:
    """
    @param: capacity: An integer
    """
    def __init__(self, capacity):
        # do intialization if necessary
        self.capacity = capacity
        self.head = Node(0,0)
        self.tail = Node(0,0)
        self.dic = {}
        self.head.next = self.tail
        self.tail.pre = self.head

    def remove(self,node):
        pre_node = node.pre
        next_node = node.next
        pre_node.next = next_node
        next_node.pre = pre_node

    def append(self,node):
        pre_node = self.tail.pre
        pre_node.next = node
        node.next = self.tail
        node.pre = pre_node
        self.tail.pre = node

    """
    @param: key: An integer
    @return: An integer
    """
    def get(self, key):
        # write your code here
        if key not in self.dic:
            return -1
        node = self.dic[key]
        self.remove(node)
        self.append(node)
        return node.value


    """
    @param: key: An integer
    @param: value: An integer
    @return: nothing
    """
    def set(self, key, value):
        # write your code here

        node = Node(key,value)
        if key in self.dic:
            old_node = self.dic[key]
            del self.dic[key]
            self.remove(old_node)
            self.append(node)
        else:
            if len(self.dic) == self.capacity:
                del_node = self.head.next
                self.remove(del_node)
                del self.dic[del_node.key]
            self.append(node)

        self.dic[key] = node
```

### Explanation
- 使用双向链表，用map保存每个node的引用。当get()时，删除节点，然后将其插入到队列尾部。
- 当set()时，存在重复的key, 将其从队列里删除，然后将新的节点插入到队列的尾部；如果没有，将队列最后的删除，插入新的元素


## Reading
本周完成mit 6.824第一个mapreduce实验。
### lab1.A doMap(),doReduce()
```go
func doMap(
	jobName string, // the name of the MapReduce job
	mapTask int, // which map task this is
	inFile string,
	nReduce int, // the number of reduce task that will be run ("R" in the paper)
	mapF func(filename string, contents string) []KeyValue,
) {
	infileChars, err := ioutil.ReadFile(inFile)
	if err != nil{
		fmt.Println("doMap open file faile .........")	
	}

	wordsMap := mapF(inFile,string(infileChars))
	enc := make([]*json.Encoder,nReduce)
	for i := 0; i < nReduce; i++{
		fname,err := os.Create(reduceName(jobName,mapTask,i))
		if err != nil{
			fmt.Println("doMap create file faile .....")
		}
		defer fname.Close() 
		enc[i] = json.NewEncoder(fname)
	}

	for _, temp := range wordsMap{
		target_file := enc[ihash(temp.Key) % nReduce]
		err := target_file.Encode(&temp)
		if err != nil{
			fmt.Println("doMap wordsMap write file fail.....")
		}

	}
```
```go
func doReduce(
	jobName string, // the name of the whole MapReduce job
	reduceTask int, // which reduce task this is
	outFile string, // write the output here
	nMap int, // the number of map tasks that were run ("M" in the paper)
	reduceF func(key string, values []string) string,
) {
	kvs := make(map[string][]string)
	for i := 0; i < nMap; i++{
		intermediateFile, err := os.Open(reduceName(jobName,i,reduceTask))
		if err != nil{
			fmt.Println("doReduce funciton open file fail .......")	
		}
		defer intermediateFile.Close()
		dec := json.NewDecoder(intermediateFile)

		for dec.More(){
			var v KeyValue 
			err := dec.Decode(&v)
			if  err != nil{
				fmt.Println("doReduce funtion dec decode file fail ....")
			}
			

			kvs[v.Key] = append(kvs[v.Key],v.Value)
		}
		defer  intermediateFile.Close()
	}

	output, err := os.Create(outFile)
	if err != nil {
		fmt.Println("creating output file failed !")
	}
	enc := json.NewEncoder(output)
	
	for key, value := range kvs{
		str := reduceF(key,value)
		fmt.Println("len of value : ",  string(len(value)))
		err := enc.Encode(&KeyValue{key,str})
		if err != nil{
			fmt.Println("write output file failed")
		}

	}
```
### lab1.B
```go
func mapF(filename string, contents string) []mapreduce.KeyValue {
	// Your code here (Part II).
//	mp := make(map[string]int)
	f := func(c rune) bool{
		return !unicode.IsLetter(c) && !unicode.IsNumber(c)
	}
	words := strings.FieldsFunc(contents,f)
	/*
	for _, v := range words{
		value, found := mp[v]
		if found{
			mp[v] = value + 1
		}else{
			mp[v] = 1
		}
	}
	res := make([]mapreduce.KeyValue,len(mp))
	index := 0
	for key, value := range mp{
		res[index] = mapreduce.KeyValue{key, string(value)}
		index++
	}
	*/
	kvs := make([]mapreduce.KeyValue,0)
	for _,w := range words{
		// one word only add 1
		kvs = append(kvs,mapreduce.KeyValue{w,"1"})
	}
	return kvs 

}

//
// The reduce function is called once for each key generated by the
// map tasks, with a list of all the values created for that key by
// any map task.
//
func reduceF(key string, values []string) string {
	// Your code here (Part II).
	sum := 0
	for _, str := range values{
		fmt.Printf(str)
		v, _ := strconv.Atoi(str)
		sum += v 
	}

	return strconv.Itoa(sum)
}
```

### lab1.C&D distributed version and fauld recovering
```go
func schedule(jobName string, mapFiles []string, nReduce int, phase jobPhase, registerChan chan string) {
	var ntasks int
	var n_other int // number of inputs (for reduce) or outputs (for map)
	switch phase {
	case mapPhase:
		ntasks = len(mapFiles)
		n_other = nReduce
	case reducePhase:
		ntasks = nReduce
		n_other = len(mapFiles)
	}

	fmt.Printf("Schedule: %v %v tasks (%d I/Os)\n", ntasks, phase, n_other)

	// All ntasks tasks have to be scheduled on workers. Once all tasks
	// have completed successfully, schedule() should return.
	//
	// Your code here (Part III, Part IV).
	//

	/*
	for i := 0; i < ntasks; i++{
		doTask := DoTaskArgs{jobName,mapFiles[i],phase,i,n_other}

		wait.Add(1)
		go func(){
			defer wait.Done()

			res := false
			for !res {
				m := <-registerChan 
				res = call(m,"worker, do task",doTask,nil)

				if res {
					registerChan <- m 
				}
			}
		}()
	}
	*/

	/*	
	var mu sync.Mutex
	produce := make(chan *DoTaskArgs,ntasks)
	consumer := make(chan string,ntasks)
	go func(){
		for i := 0; i < ntasks; i++{
			task := DoTaskArgs{jobName,mapFiles[i],phase,i,n_other}
			produce <- &task  
		}
	}()

	go func(){
		for {
		workername := <- registerChan 
		consumer <- workername 
		}
	}()

	for {
			runWork := <- consumer 
			runTask := <- produce 
			if runTask == nil{
				break
			}
	go func(){
		res := call(runWork,"do work task",*runTask,nil)
		if !res {
			fmt.Printf("schedule %v  call fail .........\n",runTask.Phase)
			produce <- runTask 
			consumer <- runWork 
			return
		}
		consumer <- runWork 

		mu.Lock()
		ntasks  = ntasks - 1;
		if ntasks <= 0{
			produce <- nil
		}

		mu.Unlock()
	
	}()
}

	*/	
	

	var wg sync.WaitGroup



	for i := 0; i < ntasks; i++ {

		taskArgs := DoTaskArgs{JobName: jobName, File: mapFiles[i], Phase: phase,

					TaskNumber: i, NumOtherPhase: n_other}



		// Tell WaitGroup to wait on one more goroutine

		wg.Add(1)



		go func() {

			defer wg.Done()

			success := false



			for !success {

				worker := <-registerChan

				success = call(worker, "Worker.DoTask", taskArgs, nil)



				if success {

					go func() { registerChan <- worker }()

				}

			}

		}()

	}



	wg.Wait()

	
	fmt.Printf("Schedule: %v done\n", phase)
}
```

### lab1.E 
```go
func mapF(document string, value string) (res []mapreduce.KeyValue) {
	// Your code here (Part V).
	kvs := make([]mapreduce.KeyValue,0)
	f := func(c rune) bool {
		return !unicode.IsLetter(c) && !unicode.IsNumber(c)
	}
	words := strings.FieldsFunc(value,f)
	set := make(map[string]bool)  
	for _, v := range words{
		_, ok := set[v] 
		if !ok {
			kvs = append(kvs,mapreduce.KeyValue{v,document})
			set[v] = true
		}
	}
	return kvs 
	
}

// The reduce function is called once for each key generated by Map, with a
// list of that key's string value (merged across all inputs). The return value
// should be a single output value for that key.
func reduceF(key string, values []string) string {
	// Your code here (Part V).
	sort.Strings(values)
	length := len(values)
	str := strings.Join(values, ",")
//	return strconv.Itoa(length) + " " + str
	return fmt.Sprintf("%d %s",length,str)

}

```

