


# ARTS
## Algorithm
[253 Meeting Room II]
Given an array of meeting time intervals consisting of start and end times `[[s1,e1],[s2,e2],...]` (si < ei), find the minimum number of conference rooms required.

For example,  
Given `[[0, 30],[5, 10],[15, 20]]`,  
return `2`.

### My Solution
```java
class Element{
        int first;
        int second;
        Element(int  _first, int _seconds){
            first = _first;
            second = _seconds;
        }
    }

    public  int minRooms ;

    public  void  help(List<Element> lists, int index, int flags[], int preSecond){
        if(index == lists.size()){
            minRooms++;
            return;
        }
        int i = index;
        for( ; i < lists.size(); i++){
            if(flags[index] == 0 && lists.get(i).first > preSecond){
                flags[i] = 1;
                break;
            }
        }
        if(i < lists.size()) {
            help(lists, i + 1, flags, lists.get(i).second);
        }else {
            minRooms++;
        }
    }

    public  int minMeetingRooms(int arr[][]){
        minRooms = 0;
        int len = arr.length;
        List<Element> lists = new ArrayList<>();
        for(int i = 0; i < len; i++){
            Element e = new Element(arr[i][0], arr[i][1]);
            lists.add(e);
        }

        Collections.sort(lists, new Comparator<Element>() {
            @Override
            public int compare(Element o1, Element o2) {
                if (o1.first == o2.first){
                    return  o1.second - o2.second;
                }
                return o1.first - o2.first;
            }
        });

        int flags[] = new int[len];
        for(int i = 0; i < len; i++){
            if(flags[i] == 0){
                help(lists,i,flags,-1);
            }
        }

        return  minRooms;
    }
```

### Other Solution
```java
class  Interval{
        int start;
        int end;
        Interval(int _start, int _end){
            start = _start;
            end = _end;
        }
    }


    /*
        code from other
     */
    public int minMeetingRooms1(Interval[] intervals) {
        if(intervals==null||intervals.length==0){
            return 0;
        }

        Comparator<Interval> comp = Comparator.comparing((Interval i)->i.start);
        Arrays.sort(intervals, comp);

        PriorityQueue<Integer> queue = new PriorityQueue<>();
        queue.offer(intervals[0].end);
        int count = 1;
        for(int i=1; i<intervals.length; i++){
            int head = queue.peek();
            if(intervals[i].start>=head){
                queue.poll();
            }else{
                count++;
            }
            queue.offer(intervals[i].end);
        }

        return count;
    }
```

### Explanation
- 首先对区间，按start 排序，然后按end排序。 
- 然后从左往右遍历，如果当前区间没有被访问，调用help(),使用深度优先遍历，找到这个room，可以放的会议。


## Reading
本周完成了mit 6.824 lab2 raft, 虽然有部分是参考别人的，通过阅读别人的代码，加深了对raft 算法细节的理解。

### leader election & log replication
```go
package raft
/*
//
// this is an outline of the API that raft must expose to
// the service (or tester). see comments below for
// each of these functions for more details.
//
// rf = Make(...)
//   create a new Raft server.
// rf.Start(command interface{}) (index, term, isleader)
//   start agreement on a new log entry
// rf.GetState() (term, isLeader)
//   ask a Raft for its current term, and whether it thinks it is leader
// ApplyMsg
//   each time a new entry is committed to the log, each Raft peer
//   should send an ApplyMsg to the service (or tester)
//   in the same server.
//

import "sync"
import "labrpc"
import "fmt"
import "time"
import "math/rand"


// import "bytes"
// import "encoding/gob"



//
// as each Raft peer becomes aware that successive log entries are
// committed, the peer should send an ApplyMsg to the service (or
// tester) on the same server, via the applyCh passed to Make().
//
type ApplyMsg struct {
	Index       int
	Command     interface{}
	UseSnapshot bool   // ignore for lab2; only used in lab3
	Snapshot    []byte // ignore for lab2; only used in lab3
}

//
// A Go object implementing a single Raft peer.
//
type Raft struct {
	mu        sync.Mutex          // Lock to protect shared access to this peer's state
	peers     []*labrpc.ClientEnd // RPC end points of all peers
	persister *Persister          // Object to hold this peer's persisted state
	me        int                 // this peer's index into peers[]

	// Your data here (2A, 2B, 2C).
	// Look at the paper's Figure 2 for a description of what
	// state a Raft server must maintain.
	currentTerm int 
	votedIndex int 
	votedNum int 
	status int 
	logs [] LogEntry

	voteCh chan bool
	heartBreak chan bool
	electedCh chan bool
	applyCh chan ApplyMsg

	commitIndex int 
	lastApplied int 
	
	nextIndex []int
	matchIndex []int 

}
type LogEntry struct{
	term int 
	command interface{}
}

const (
	LEADER = 0
	CANDIDATE = 1
	FOLLOWER = 2
)

func (rf *Raft) getLastLogIndex() int {
	return len(rf.logs) - 1
}

func (rf *Raft) getLastLogTerm() int{
	return rf.logs[len(rf.logs) - 1].term
}

// return currentTerm and whether this server
// believes it is the leader.
func (rf *Raft) GetState() (int, bool) {

	var term int
	var isleader bool
	term = rf.currentTerm
	isleader = rf.status == LEADER

	// Your code here (2A).
	return term, isleader
}

//
// save Raft's persistent state to stable storage,
// where it can later be retrieved after a crash and restart.
// see paper's Figure 2 for a description of what should be persistent.
//
func (rf *Raft) persist() {
	// Your code here (2C).
	// Example:
	// w := new(bytes.Buffer)
	// e := gob.NewEncoder(w)
	// e.Encode(rf.xxx)
	// e.Encode(rf.yyy)
	// data := w.Bytes()
	// rf.persister.SaveRaftState(data)
}

//
// restore previously persisted state.
//
func (rf *Raft) readPersist(data []byte) {
	// Your code here (2C).
	// Example:
	// r := bytes.NewBuffer(data)
	// d := gob.NewDecoder(r)
	// d.Decode(&rf.xxx)
	// d.Decode(&rf.yyy)
	if data == nil || len(data) < 1 { // bootstrap without any state?
		return
	}
}




//
// example RequestVote RPC arguments structure.
// field names must start with capital letters!
//
type RequestVoteArgs struct {
	// Your data here (2A, 2B).
	term int 
	candidateId int 
	preLastLogIndex int 
	preLastLogTerm int 
}

//
// example RequestVote RPC reply structure.
// field names must start with capital letters!
//
type RequestVoteReply struct {
	// Your data here (2A).
	term int 
	success  bool 
}

//
// example RequestVote RPC handler.
//

//follower deal with vote request
func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) {
	// Your code here (2A, 2B).
	fmt.Printf("follower %v is dealing with vote request from candidate %v \n", rf.me,args.candidateId)
	rf.mu.Lock()
	defer rf.mu.Unlock()
	if args.term < rf.currentTerm{
		reply.term = rf.currentTerm
		reply.success = false
		fmt.Printf("follower %v reject vote for candidate %v \n", rf.me, args.candidateId)
		return
	}
	if args.term > rf.currentTerm{
		rf.currentTerm = args.term
		rf.status = FOLLOWER
		rf.votedIndex = -1
	}


	reply.term = rf.currentTerm
	reply.success = false

	// there is case rf.term == rf.currentTerm, and together candidate
	// the leader will be bigger term or same term and bigger lastIndex
	b := (args.preLastLogTerm > rf.getLastLogTerm()) || (args.preLastLogIndex >= rf.getLastLogIndex())
	if (rf.votedIndex == -1 || rf.votedIndex == args.candidateId) && b{
		reply.success = true
		rf.votedIndex = args.candidateId

		fmt.Printf("follower %v success voted candidate %v \n", rf.me,args.candidateId)
		// later not permit deal with vote request
		rf.voteCh <- true
	}

}

//
// example code to send a RequestVote RPC to a server.
// server is the index of the target server in rf.peers[].
// expects RPC arguments in args.
// fills in *reply with RPC reply, so caller should
// pass &reply.
// the types of the args and reply passed to Call() must be
// the same as the types of the arguments declared in the
// handler function (including whether they are pointers).
//
// The labrpc package simulates a lossy network, in which servers
// may be unreachable, and in which requests and replies may be lost.
// Call() sends a request and waits for a reply. If a reply arrives
// within a timeout interval, Call() returns true; otherwise
// Call() returns false. Thus Call() may not return for a while.
// A false return can be caused by a dead server, a live server that
// can't be reached, a lost request, or a lost reply.
//
// Call() is guaranteed to return (perhaps after a delay) *except* if the
// handler function on the server side does not return.  Thus there
// is no need to implement your own timeouts around Call().
//
// look at the comments in ../labrpc/labrpc.go for more details.
//
// if you're having trouble getting RPC to work, check that you've
// capitalized all field names in structs passed over RPC, and
// that the caller passes the address of the reply struct with &, not
// the struct itself.
//

//candidate send vote request to follower
func (rf *Raft) sendRequestVote(server int, args *RequestVoteArgs, reply *RequestVoteReply) bool {

	fmt.Printf("candidate %v send vote request to follower %v \n", rf.me, server)
	ok := rf.peers[server].Call("Raft.RequestVote", args, reply)
	return ok

	if !ok || rf.me != args.candidateId || rf.currentTerm != args.term{
		fmt.Printf("candiate %v send invalid vote request to follower %v \n", rf.me,server)
		return ok 
	}

	if reply.term > rf.currentTerm{
		rf.status = FOLLOWER
		rf.votedIndex = -1
		rf.votedNum = 0
		fmt.Printf("candiate %v current term smaller to follower %v",rf.me, server)
		return ok 
	}

	if reply.success{
		rf.votedNum++
		if rf.votedNum > len(rf.peers) / 2{
			fmt.Printf("election complete, candiate %v become leader", rf.me)
			rf.status = LEADER
			rf.electedCh <- true
		
		}
	}
	return ok 
}

func (rf *Raft) broadcastRequestVote(){
	rf.mu.Lock()
	args := RequestVoteArgs{term:rf.currentTerm,candidateId:rf.me,
			preLastLogIndex: rf.getLastLogIndex(),preLastLogTerm:rf.getLastLogTerm()}
	rf.mu.Unlock()

	fmt.Printf("candidate %v broadcast vote reqest to all server \n", rf.me)
	for i := range rf.peers{
		if i != rf.me && rf.status == CANDIDATE{
			fmt.Printf("Candidate %v is sending to vote request to server %v \n", rf.me, i)
			go rf.sendRequestVote(i,&args,&RequestVoteReply{})
		}
	}
}

type AppendEntriesArgs struct{
	term int 
	leaderId int 
	preLastLogIndex int 
	preLastLogTerm int 
	entries []LogEntry
	leaderCommit int 
}

type AppendEntriesReply struct{
	term int 
	success bool 
	followerLogNextIndex int 
}

// follower deal with log entries
func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply){

	fmt.Printf("follower %v is dealing with log entries from leaderId %v \n", rf.me, args.leaderId)
	if args.term < rf.currentTerm{
		reply.term = rf.currentTerm
		reply.success = false
		reply.followerLogNextIndex = rf.getLastLogIndex() + 1
		fmt.Printf("follower %v reject append log entries from leaderId %v \n", rf.me, args.leaderId)
		return
	}
	if args.term > rf.currentTerm{
		rf.currentTerm = args.term
		rf.votedIndex = -1
		rf.status = FOLLOWER
	}
	reply.success = false
	reply.term = rf.currentTerm

	// not permmit receive heartBreak
	rf.heartBreak <- true

	if args.preLastLogIndex > rf.getLastLogIndex(){
		reply.followerLogNextIndex = rf.getLastLogIndex() +1
		return
	}

	// preIndexLogIndex, represent leader store follower i ,the last log entried idnex
	
	if args.preLastLogIndex > 0 && rf.logs[args.preLastLogIndex].term != args.term{
		index := args.preLastLogIndex -1 
		for ; index >= 0; index-- {
			if rf.logs[index].term != rf.logs[args.preLastLogIndex].term{
				break
			}
		}
		reply.followerLogNextIndex = index
	}else{
		preLog := rf.logs[:args.preLastLogIndex+1]
		resLog := rf.logs[args.preLastLogIndex+1:]
		rf.logs = preLog

		if rf.checkConfilct(resLog,args.entries) || len(resLog) < len(args.entries){
			rf.logs = append(rf.logs, args.entries...)
		}else{
			rf.logs = append(rf.logs, resLog...)
		}

		reply.success = true
		reply.followerLogNextIndex = args.preLastLogIndex

		if args.leaderCommit > rf.commitIndex{
			if args.leaderCommit < rf.getLastLogIndex(){
				rf.commitIndex = args.leaderCommit
			}else{
				rf.commitIndex = rf.getLastLogIndex()
			}
			fmt.Printf("follower %v commit log \n", rf.me)
			go rf.commitLog()
		}

	}

}

func (rf *Raft) commitLog(){
	rf.mu.Lock()
	defer rf.mu.Unlock()

	for i := rf.lastApplied + 1; i <= rf.commitIndex; i++{
		rf.applyCh <- ApplyMsg{Index:i, Command : rf.logs[i].command}
	}

	rf.lastApplied = rf.commitIndex
}

func (rf *Raft) checkConfilct(followerLogs []LogEntry,appednLogs []LogEntry)bool{
	if len(followerLogs) > len(appednLogs){
		return false
	}
	for i := range followerLogs{
		if followerLogs[i].term != appednLogs[i].term{
			return true
		}
	}
	return false
}


// leader send log entries
func (rf *Raft) sendAppendEntries(server int, args *AppendEntriesArgs, reply * AppendEntriesReply) bool{
	fmt.Printf("leader %v send log entries to server %v \n", rf.me, server)
	ok := rf.peers[server].Call("Raft.AppendEntries", args, reply)
	rf.mu.Lock()

	defer rf.mu.Unlock()

	if !ok || rf.status != LEADER || args.term != rf.currentTerm{
		return ok 
	} 

	if reply.term > rf.currentTerm{
		rf.currentTerm = reply.term
		rf.status = FOLLOWER
		rf.votedIndex = -1

		fmt.Printf("leader %v set to follower in sendAppendEntries \n", rf.me)
		return ok 
	}

	if reply.success{
		rf.matchIndex[server] = args.preLastLogIndex + len(args.entries)
		rf.nextIndex[server] = rf.matchIndex[server] -1
	}else{
		rf.nextIndex[server] = reply.followerLogNextIndex
	}

	// leader commit applied log

	for N := rf.getLastLogIndex(); N > rf.commitIndex; N--{
		count := 1
		if rf.logs[N].term == rf.currentTerm {
			for i := range rf.peers{
				if rf.matchIndex[i] >= N{
					count++
				}
			}
		}
		if count > len(rf.peers) / 2{
			rf.commitIndex = N 
			go rf.commitLog() 
			break
		}
	}

	return ok 
}


func (rf *Raft) broadcastAppendEntries(){
	rf.mu.Lock()
	defer rf.mu.Unlock()

	for i := range rf.peers{
		if i != rf.me && rf.status == LEADER{
			args := AppendEntriesArgs{}
			args.term = rf.currentTerm
			args.leaderId = rf.me
			args.preLastLogIndex = rf.nextIndex[i]  -1 
			args.leaderCommit = rf.commitIndex

			if args.preLastLogIndex >= 0{
				args.preLastLogTerm = rf.logs[args.preLastLogIndex].term
			}

			if rf.nextIndex[i] <= rf.getLastLogIndex(){
				args.entries = rf.logs[rf.nextIndex[i]:]
			}
			go rf.sendAppendEntries(i,&args, &AppendEntriesReply{})
		}
	}
}


//
// the service using Raft (e.g. a k/v server) wants to start
// agreement on the next command to be appended to Raft's log. if this
// server isn't the leader, returns false. otherwise start the
// agreement and return immediately. there is no guarantee that this
// command will ever be committed to the Raft log, since the leader
// may fail or lose an election.
//
// the first return value is the index that the command will appear at
// if it's ever committed. the second return value is the current
// term. the third return value is true if this server believes it is
// the leader.
//
func (rf *Raft) Start(command interface{}) (int, int, bool) {
	index := -1
	term := -1
	isLeader := rf.status == LEADER
	if isLeader {
		index = rf.getLastLogIndex() +1 
		term = rf.currentTerm
		rf.logs = append(rf.logs, LogEntry{term,command})
	}

	// Your code here (2B).


	return index, term, isLeader
}

//
// the tester calls Kill() when a Raft instance won't
// be needed again. you are not required to do anything
// in Kill(), but it might be convenient to (for example)
// turn off debug output from this instance.
//
func (rf *Raft) Kill() {
	// Your code here, if desired.
}

//
// the service or tester wants to create a Raft server. the ports
// of all the Raft servers (including this one) are in peers[]. this
// server's port is peers[me]. all the servers' peers[] arrays
// have the same order. persister is a place for this server to
// save its persistent state, and also initially holds the most
// recent saved state, if any. applyCh is a channel on which the
// tester or service expects Raft to send ApplyMsg messages.
// Make() must return quickly, so it should start goroutines
// for any long-running work.
//


func (rf *Raft) runServer(){
	for {
		switch rf.status {
		case LEADER :
			rf.broadcastAppendEntries()
			time.Sleep(time.Millisecond * 120)
	case FOLLOWER:
		select{
		case <-rf.voteCh:
		case <- rf.heartBreak:
		case<- time.After(time.Millisecond * time.Duration(rand.Intn(200) + 300)):
			rf.status = CANDIDATE
		}
	case CANDIDATE:
		rf.mu.Lock()
		rf.currentTerm++
		rf.votedIndex = rf.me
		rf.votedNum = 1
		rf.mu.Unlock()
		rf.broadcastRequestVote()

		select{
		case <-time.After(time.Millisecond * time.Duration(rand.Intn(200) + 300)):
		case <- rf.heartBreak:
			rf.status = FOLLOWER
		case <- rf.electedCh:
			rf.mu.Lock()
			rf.status = LEADER
			rf.nextIndex = make([]int, len(rf.peers))
			rf.matchIndex = make([]int, len(rf.peers))
			nextIdx := rf.getLastLogIndex() + 1
			for i := range rf.peers{
				rf.nextIndex[i] = nextIdx
			}

			rf.mu.Unlock()
		}
		}
	}
}
func Make(peers []*labrpc.ClientEnd, me int,
	persister *Persister, applyCh chan ApplyMsg) *Raft {
	rf := &Raft{}
	rf.peers = peers
	rf.persister = persister
	rf.me = me

	rf.currentTerm = 0
	rf.votedIndex = -1
	rf.votedNum = 0
	rf.commitIndex = 0
	rf.lastApplied = 0
	rf.status = FOLLOWER
	rf.applyCh = applyCh
	rf.electedCh = make(chan bool)
	rf.voteCh = make(chan bool)
	rf.heartBreak = make(chan bool)
	rf.logs = append(rf.logs,LogEntry{term:0})

	// Your initialization code here (2A, 2B, 2C).

	// initialize from state persisted before a crash
	rf.readPersist(persister.ReadRaftState())

	go rf.runServer()
	return rf
}
*/

```
- 自己实现的时候，第一次遇到null pointer, 解决后，发现rpc 远程调用过程中，reply decode 时，一直发现错误， 前前后后大概花了4天 debug,最后放弃了。因为后面需要用到lab2的代码，就使用了别人的。

```go
package raft
/*
//
// this is an outline of the API that raft must expose to
// the service (or tester). see comments below for
// each of these functions for more details.
//
// rf = Make(...)
//   create a new Raft server.
// rf.Start(command interface{}) (index, term, isleader)
//   start agreement on a new log entry
// rf.GetState() (term, isLeader)
//   ask a Raft for its current term, and whether it thinks it is leader
// ApplyMsg
//   each time a new entry is committed to the log, each Raft peer
//   should send an ApplyMsg to the service (or tester)
//   in the same server.
//

import "sync"
import "labrpc"
import "fmt"
import "time"
import "math/rand"


// import "bytes"
// import "encoding/gob"



//
// as each Raft peer becomes aware that successive log entries are
// committed, the peer should send an ApplyMsg to the service (or
// tester) on the same server, via the applyCh passed to Make().
//
type ApplyMsg struct {
	Index       int
	Command     interface{}
	UseSnapshot bool   // ignore for lab2; only used in lab3
	Snapshot    []byte // ignore for lab2; only used in lab3
}

//
// A Go object implementing a single Raft peer.
//
type Raft struct {
	mu        sync.Mutex          // Lock to protect shared access to this peer's state
	peers     []*labrpc.ClientEnd // RPC end points of all peers
	persister *Persister          // Object to hold this peer's persisted state
	me        int                 // this peer's index into peers[]

	// Your data here (2A, 2B, 2C).
	// Look at the paper's Figure 2 for a description of what
	// state a Raft server must maintain.
	currentTerm int 
	votedIndex int 
	votedNum int 
	status int 
	logs [] LogEntry

	voteCh chan bool
	heartBreak chan bool
	electedCh chan bool
	applyCh chan ApplyMsg

	commitIndex int 
	lastApplied int 
	
	nextIndex []int
	matchIndex []int 

}
type LogEntry struct{
	term int 
	command interface{}
}

const (
	LEADER = 0
	CANDIDATE = 1
	FOLLOWER = 2
)

func (rf *Raft) getLastLogIndex() int {
	return len(rf.logs) - 1
}

func (rf *Raft) getLastLogTerm() int{
	return rf.logs[len(rf.logs) - 1].term
}

// return currentTerm and whether this server
// believes it is the leader.
func (rf *Raft) GetState() (int, bool) {

	var term int
	var isleader bool
	term = rf.currentTerm
	isleader = rf.status == LEADER

	// Your code here (2A).
	return term, isleader
}

//
// save Raft's persistent state to stable storage,
// where it can later be retrieved after a crash and restart.
// see paper's Figure 2 for a description of what should be persistent.
//
func (rf *Raft) persist() {
	// Your code here (2C).
	// Example:
	// w := new(bytes.Buffer)
	// e := gob.NewEncoder(w)
	// e.Encode(rf.xxx)
	// e.Encode(rf.yyy)
	// data := w.Bytes()
	// rf.persister.SaveRaftState(data)
}

//
// restore previously persisted state.
//
func (rf *Raft) readPersist(data []byte) {
	// Your code here (2C).
	// Example:
	// r := bytes.NewBuffer(data)
	// d := gob.NewDecoder(r)
	// d.Decode(&rf.xxx)
	// d.Decode(&rf.yyy)
	if data == nil || len(data) < 1 { // bootstrap without any state?
		return
	}
}




//
// example RequestVote RPC arguments structure.
// field names must start with capital letters!
//
type RequestVoteArgs struct {
	// Your data here (2A, 2B).
	term int 
	candidateId int 
	preLastLogIndex int 
	preLastLogTerm int 
}

//
// example RequestVote RPC reply structure.
// field names must start with capital letters!
//
type RequestVoteReply struct {
	// Your data here (2A).
	term int 
	success  bool 
}

//
// example RequestVote RPC handler.
//

//follower deal with vote request
func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) {
	// Your code here (2A, 2B).
	fmt.Printf("follower %v is dealing with vote request from candidate %v \n", rf.me,args.candidateId)
	rf.mu.Lock()
	defer rf.mu.Unlock()
	if args.term < rf.currentTerm{
		reply.term = rf.currentTerm
		reply.success = false
		fmt.Printf("follower %v reject vote for candidate %v \n", rf.me, args.candidateId)
		return
	}
	if args.term > rf.currentTerm{
		rf.currentTerm = args.term
		rf.status = FOLLOWER
		rf.votedIndex = -1
	}


	reply.term = rf.currentTerm
	reply.success = false

	// there is case rf.term == rf.currentTerm, and together candidate
	// the leader will be bigger term or same term and bigger lastIndex
	b := (args.preLastLogTerm > rf.getLastLogTerm()) || (args.preLastLogIndex >= rf.getLastLogIndex())
	if (rf.votedIndex == -1 || rf.votedIndex == args.candidateId) && b{
		reply.success = true
		rf.votedIndex = args.candidateId

		fmt.Printf("follower %v success voted candidate %v \n", rf.me,args.candidateId)
		// later not permit deal with vote request
		rf.voteCh <- true
	}

}

//
// example code to send a RequestVote RPC to a server.
// server is the index of the target server in rf.peers[].
// expects RPC arguments in args.
// fills in *reply with RPC reply, so caller should
// pass &reply.
// the types of the args and reply passed to Call() must be
// the same as the types of the arguments declared in the
// handler function (including whether they are pointers).
//
// The labrpc package simulates a lossy network, in which servers
// may be unreachable, and in which requests and replies may be lost.
// Call() sends a request and waits for a reply. If a reply arrives
// within a timeout interval, Call() returns true; otherwise
// Call() returns false. Thus Call() may not return for a while.
// A false return can be caused by a dead server, a live server that
// can't be reached, a lost request, or a lost reply.
//
// Call() is guaranteed to return (perhaps after a delay) *except* if the
// handler function on the server side does not return.  Thus there
// is no need to implement your own timeouts around Call().
//
// look at the comments in ../labrpc/labrpc.go for more details.
//
// if you're having trouble getting RPC to work, check that you've
// capitalized all field names in structs passed over RPC, and
// that the caller passes the address of the reply struct with &, not
// the struct itself.
//

//candidate send vote request to follower
func (rf *Raft) sendRequestVote(server int, args *RequestVoteArgs, reply *RequestVoteReply) bool {

	fmt.Printf("candidate %v send vote request to follower %v \n", rf.me, server)
	ok := rf.peers[server].Call("Raft.RequestVote", args, reply)
	return ok

	if !ok || rf.me != args.candidateId || rf.currentTerm != args.term{
		fmt.Printf("candiate %v send invalid vote request to follower %v \n", rf.me,server)
		return ok 
	}

	if reply.term > rf.currentTerm{
		rf.status = FOLLOWER
		rf.votedIndex = -1
		rf.votedNum = 0
		fmt.Printf("candiate %v current term smaller to follower %v",rf.me, server)
		return ok 
	}

	if reply.success{
		rf.votedNum++
		if rf.votedNum > len(rf.peers) / 2{
			fmt.Printf("election complete, candiate %v become leader", rf.me)
			rf.status = LEADER
			rf.electedCh <- true
		
		}
	}
	return ok 
}

func (rf *Raft) broadcastRequestVote(){
	rf.mu.Lock()
	args := RequestVoteArgs{term:rf.currentTerm,candidateId:rf.me,
			preLastLogIndex: rf.getLastLogIndex(),preLastLogTerm:rf.getLastLogTerm()}
	rf.mu.Unlock()

	fmt.Printf("candidate %v broadcast vote reqest to all server \n", rf.me)
	for i := range rf.peers{
		if i != rf.me && rf.status == CANDIDATE{
			fmt.Printf("Candidate %v is sending to vote request to server %v \n", rf.me, i)
			go rf.sendRequestVote(i,&args,&RequestVoteReply{})
		}
	}
}

type AppendEntriesArgs struct{
	term int 
	leaderId int 
	preLastLogIndex int 
	preLastLogTerm int 
	entries []LogEntry
	leaderCommit int 
}

type AppendEntriesReply struct{
	term int 
	success bool 
	followerLogNextIndex int 
}

// follower deal with log entries
func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply){

	fmt.Printf("follower %v is dealing with log entries from leaderId %v \n", rf.me, args.leaderId)
	if args.term < rf.currentTerm{
		reply.term = rf.currentTerm
		reply.success = false
		reply.followerLogNextIndex = rf.getLastLogIndex() + 1
		fmt.Printf("follower %v reject append log entries from leaderId %v \n", rf.me, args.leaderId)
		return
	}
	if args.term > rf.currentTerm{
		rf.currentTerm = args.term
		rf.votedIndex = -1
		rf.status = FOLLOWER
	}
	reply.success = false
	reply.term = rf.currentTerm

	// not permmit receive heartBreak
	rf.heartBreak <- true

	if args.preLastLogIndex > rf.getLastLogIndex(){
		reply.followerLogNextIndex = rf.getLastLogIndex() +1
		return
	}

	// preIndexLogIndex, represent leader store follower i ,the last log entried idnex
	
	if args.preLastLogIndex > 0 && rf.logs[args.preLastLogIndex].term != args.term{
		index := args.preLastLogIndex -1 
		for ; index >= 0; index-- {
			if rf.logs[index].term != rf.logs[args.preLastLogIndex].term{
				break
			}
		}
		reply.followerLogNextIndex = index
	}else{
		preLog := rf.logs[:args.preLastLogIndex+1]
		resLog := rf.logs[args.preLastLogIndex+1:]
		rf.logs = preLog

		if rf.checkConfilct(resLog,args.entries) || len(resLog) < len(args.entries){
			rf.logs = append(rf.logs, args.entries...)
		}else{
			rf.logs = append(rf.logs, resLog...)
		}

		reply.success = true
		reply.followerLogNextIndex = args.preLastLogIndex

		if args.leaderCommit > rf.commitIndex{
			if args.leaderCommit < rf.getLastLogIndex(){
				rf.commitIndex = args.leaderCommit
			}else{
				rf.commitIndex = rf.getLastLogIndex()
			}
			fmt.Printf("follower %v commit log \n", rf.me)
			go rf.commitLog()
		}

	}

}

func (rf *Raft) commitLog(){
	rf.mu.Lock()
	defer rf.mu.Unlock()

	for i := rf.lastApplied + 1; i <= rf.commitIndex; i++{
		rf.applyCh <- ApplyMsg{Index:i, Command : rf.logs[i].command}
	}

	rf.lastApplied = rf.commitIndex
}

func (rf *Raft) checkConfilct(followerLogs []LogEntry,appednLogs []LogEntry)bool{
	if len(followerLogs) > len(appednLogs){
		return false
	}
	for i := range followerLogs{
		if followerLogs[i].term != appednLogs[i].term{
			return true
		}
	}
	return false
}


// leader send log entries
func (rf *Raft) sendAppendEntries(server int, args *AppendEntriesArgs, reply * AppendEntriesReply) bool{
	fmt.Printf("leader %v send log entries to server %v \n", rf.me, server)
	ok := rf.peers[server].Call("Raft.AppendEntries", args, reply)
	rf.mu.Lock()

	defer rf.mu.Unlock()

	if !ok || rf.status != LEADER || args.term != rf.currentTerm{
		return ok 
	} 

	if reply.term > rf.currentTerm{
		rf.currentTerm = reply.term
		rf.status = FOLLOWER
		rf.votedIndex = -1

		fmt.Printf("leader %v set to follower in sendAppendEntries \n", rf.me)
		return ok 
	}

	if reply.success{
		rf.matchIndex[server] = args.preLastLogIndex + len(args.entries)
		rf.nextIndex[server] = rf.matchIndex[server] -1
	}else{
		rf.nextIndex[server] = reply.followerLogNextIndex
	}

	// leader commit applied log

	for N := rf.getLastLogIndex(); N > rf.commitIndex; N--{
		count := 1
		if rf.logs[N].term == rf.currentTerm {
			for i := range rf.peers{
				if rf.matchIndex[i] >= N{
					count++
				}
			}
		}
		if count > len(rf.peers) / 2{
			rf.commitIndex = N 
			go rf.commitLog() 
			break
		}
	}

	return ok 
}


func (rf *Raft) broadcastAppendEntries(){
	rf.mu.Lock()
	defer rf.mu.Unlock()

	for i := range rf.peers{
		if i != rf.me && rf.status == LEADER{
			args := AppendEntriesArgs{}
			args.term = rf.currentTerm
			args.leaderId = rf.me
			args.preLastLogIndex = rf.nextIndex[i]  -1 
			args.leaderCommit = rf.commitIndex

			if args.preLastLogIndex >= 0{
				args.preLastLogTerm = rf.logs[args.preLastLogIndex].term
			}

			if rf.nextIndex[i] <= rf.getLastLogIndex(){
				args.entries = rf.logs[rf.nextIndex[i]:]
			}
			go rf.sendAppendEntries(i,&args, &AppendEntriesReply{})
		}
	}
}


//
// the service using Raft (e.g. a k/v server) wants to start
// agreement on the next command to be appended to Raft's log. if this
// server isn't the leader, returns false. otherwise start the
// agreement and return immediately. there is no guarantee that this
// command will ever be committed to the Raft log, since the leader
// may fail or lose an election.
//
// the first return value is the index that the command will appear at
// if it's ever committed. the second return value is the current
// term. the third return value is true if this server believes it is
// the leader.
//
func (rf *Raft) Start(command interface{}) (int, int, bool) {
	index := -1
	term := -1
	isLeader := rf.status == LEADER
	if isLeader {
		index = rf.getLastLogIndex() +1 
		term = rf.currentTerm
		rf.logs = append(rf.logs, LogEntry{term,command})
	}

	// Your code here (2B).


	return index, term, isLeader
}

//
// the tester calls Kill() when a Raft instance won't
// be needed again. you are not required to do anything
// in Kill(), but it might be convenient to (for example)
// turn off debug output from this instance.
//
func (rf *Raft) Kill() {
	// Your code here, if desired.
}

//
// the service or tester wants to create a Raft server. the ports
// of all the Raft servers (including this one) are in peers[]. this
// server's port is peers[me]. all the servers' peers[] arrays
// have the same order. persister is a place for this server to
// save its persistent state, and also initially holds the most
// recent saved state, if any. applyCh is a channel on which the
// tester or service expects Raft to send ApplyMsg messages.
// Make() must return quickly, so it should start goroutines
// for any long-running work.
//


func (rf *Raft) runServer(){
	for {
		switch rf.status {
		case LEADER :
			rf.broadcastAppendEntries()
			time.Sleep(time.Millisecond * 120)
	case FOLLOWER:
		select{
		case <-rf.voteCh:
		case <- rf.heartBreak:
		case<- time.After(time.Millisecond * time.Duration(rand.Intn(200) + 300)):
			rf.status = CANDIDATE
		}
	case CANDIDATE:
		rf.mu.Lock()
		rf.currentTerm++
		rf.votedIndex = rf.me
		rf.votedNum = 1
		rf.mu.Unlock()
		rf.broadcastRequestVote()

		select{
		case <-time.After(time.Millisecond * time.Duration(rand.Intn(200) + 300)):
		case <- rf.heartBreak:
			rf.status = FOLLOWER
		case <- rf.electedCh:
			rf.mu.Lock()
			rf.status = LEADER
			rf.nextIndex = make([]int, len(rf.peers))
			rf.matchIndex = make([]int, len(rf.peers))
			nextIdx := rf.getLastLogIndex() + 1
			for i := range rf.peers{
				rf.nextIndex[i] = nextIdx
			}

			rf.mu.Unlock()
		}
		}
	}
}
func Make(peers []*labrpc.ClientEnd, me int,
	persister *Persister, applyCh chan ApplyMsg) *Raft {
	rf := &Raft{}
	rf.peers = peers
	rf.persister = persister
	rf.me = me

	rf.currentTerm = 0
	rf.votedIndex = -1
	rf.votedNum = 0
	rf.commitIndex = 0
	rf.lastApplied = 0
	rf.status = FOLLOWER
	rf.applyCh = applyCh
	rf.electedCh = make(chan bool)
	rf.voteCh = make(chan bool)
	rf.heartBreak = make(chan bool)
	rf.logs = append(rf.logs,LogEntry{term:0})

	// Your initialization code here (2A, 2B, 2C).

	// initialize from state persisted before a crash
	rf.readPersist(persister.ReadRaftState())

	go rf.runServer()
	return rf
}
*/

``` 
主要参考 
1. [liu-jianhao](https://liu-jianhao.github.io/2018/11/mit6.824分布式系统实验二之raft/
)
2. [WenbinZhu](https://github.com/WenbinZhu/MIT-6.824-labs/blob/master/src/raft/raft.go#L304
)
